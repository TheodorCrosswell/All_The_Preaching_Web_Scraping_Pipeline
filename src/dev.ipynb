{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a626647a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import webvtt\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import polars as pl\n",
    "import patito as pt\n",
    "import json\n",
    "from chromadb import QueryResult, PersistentClient, Collection, Settings\n",
    "import numpy as np\n",
    "import os\n",
    "from pydantic import BaseModel, Field, TypeAdapter, ValidationError\n",
    "import pytest\n",
    "import helpers\n",
    "from helpers import video_url_pattern, mp4_url_pattern, mp3_url_pattern, vtt_url_pattern\n",
    "from enum import Enum\n",
    "from typing import Optional\n",
    "from tqdm import tqdm\n",
    "from config import *\n",
    "import semchunk\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146bbef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "# chroma_db_path = os.path.join(project_root, \".chroma\")\n",
    "\n",
    "chroma_db_path = r\"C:\\repos\\All_The_Preaching_Web_Scraping_Pipeline\\.chroma\" #temporary while using notebook\n",
    "\n",
    "client = PersistentClient(\n",
    "    path=chroma_db_path,\n",
    "    settings=Settings(\n",
    "        is_persistent=True,\n",
    "        persist_directory=chroma_db_path,\n",
    "        anonymized_telemetry=False,\n",
    "    ),\n",
    ")\n",
    "\n",
    "client.delete_collection(\"atp_transcripts\")\n",
    "# transcript_collection = client.get_or_create_collection(name=\"atp_transcripts\", embedding_function=embedding_function)\n",
    "\n",
    "chunk_collection = client.get_or_create_collection(name=\"atp_chunks\")\n",
    "\n",
    "print(chunk_collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5e4457",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    helpers.TranscriptDataFrameModel.LazyFrame(\n",
    "    pl.read_parquet(\n",
    "        r\"C:\\repos\\All_The_Preaching_Web_Scraping_Pipeline\\data\\transcript.parquet\"\n",
    "    )\n",
    "    )\n",
    "    #\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "try:\n",
    "    df.validate()\n",
    "except pt.DataFrameValidationError as e:\n",
    "    print(e)\n",
    "    # raise(e)\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f7dd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_chunked_record_df(df_transcript: pt.DataFrame) -> pt.DataFrame:\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    chunk_overlap = 30\n",
    "    chunk_size = 256  # max tokens per chunk\n",
    "    chunker = semchunk.chunkerify(tokenizer, chunk_size)\n",
    "    df_chunks = helpers.ChunkedRecordDataFrameModel.DataFrame()\n",
    "    for i, transcript in enumerate(df_transcript[\"transcript\"]):\n",
    "        video_id = df_transcript.item(i,0)\n",
    "        chunks = chunker(transcript, overlap=chunk_overlap)\n",
    "        indexes = list(range(1,len(chunks) + 1))\n",
    "        token_counts = [len(tokenizer.tokenize(chunk)) for chunk in chunks]\n",
    "        chunks_count = [len(chunks) for chunk in chunks]\n",
    "        df_chunks.vstack(\n",
    "            pl.DataFrame(\n",
    "                {\n",
    "                    \"chunk\": chunks,\n",
    "                    \"chunk_number\": indexes,\n",
    "                    \"token_count\": token_counts,\n",
    "                    \"chunks_count\": chunks_count,\n",
    "                }\n",
    "            )\n",
    "            .join(\n",
    "                df_transcript\n",
    "                .filter(pl.col(\"video_id\").eq(video_id))\n",
    "                .drop([\"vtt\", \"transcript\", \"mp3_url\", \"vtt_url\", \"transcript_hash\"])\n",
    "                , how=\"cross\"\n",
    "            )\n",
    "            , in_place=True\n",
    "        )\n",
    "    df_chunks = (\n",
    "        df_chunks.with_columns(\n",
    "            (pl.col(\"video_id\").cast(pl.String) + pl.lit(\"_\") + \n",
    "            pl.col(\"chunk_number\").cast(pl.String))\n",
    "            .alias(\"chunk_id\")\n",
    "        )\n",
    "    )\n",
    "    try:\n",
    "        df_chunks.validate()\n",
    "    except pt.DataFrameValidationError as e:\n",
    "        print(e)\n",
    "        # raise(e)\n",
    "    return df_chunks\n",
    "    # (Pandas version, irrelevant now) took 48.5 minutes to run with 16122 transcripts, 30 overlap, 256 size, resulting in 985480 chunks\n",
    "    # Projected to take 90 minutes with 16k transcripts, have not tested. Maybe it's slower due to patito validation.\n",
    "to_chunked_record_df(df.head(20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
